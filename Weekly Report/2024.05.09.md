论文阅读：Fine-tuning Language Models for Factuality（针对事实性对语言模型进行微调）
- 研究动机：
	- 仍然是为了解决幻觉问题
	- 对模型的响应进行手动检查事实性是一个耗时的过程，同时产生很高人力成本
- 创新点：
	- 最近的工作（2023.11）之前使用测量与外部知识库的一致性或大模型的置信度来判断开放性文本的真实性，直接偏好优化算法能够使用对可能的模型响应的偏好排序，在监督模仿之外的目标直接微调语言模型
	- 本文提出与RLHF或以事实为目标的解码策略相比，从现有的检索系统或本文提出的无检索方法自动生成的事实偏好排名中学习，可以显著提高llama2在未决主题上的真实性
	- 在生成传记或回答医学问题时，事实错误率分别降低了58%和40%
- 引言：
	- 大语言模型很容易自信地给出不正确的声明和引用
	- 1. 比如询问大模型“Yo-Yo Ma的出生地”，大模型会确定地生成“idk，可能是巴黎”
	- 但是如果预训练数据包含对该问题的其他回答，模型会产生极高的损失
	- 2. 如果大模型在回答问题的时候不只给出一个确定的答案，而是对多种可能的答案都给出一定的概率，那么在训练时可能得到更低的损失，并且模型更倾向于输出模糊回答，可能会导致一定程度上不准确的信息
	- 原则上可以选择更好的奖励函数避免这个问题，但是精确计算的成本很高，人工标记的时间成本也很高
	- 本文利用了在没有人工干预下估计真实性的最新进展：
		- 使用基于参考的自动事实检查方法，评估外部知识库对文本的支持程度【】
		- 使用模型自身的置信度作为真实性的代理，作为无参考的真实性评估【】
	- 本文使用这些真实性度量和未标记提示的数据集，使用预训练的模型采样，用偏好标签进行注释，使用直接偏好优化算法DPO【】
	- 总结：提出了一种直接的方式来优化语言模型，实现长文本的真实性，无需人工注释
	- ![image.png](https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository@main/picgo/202405081424292.png)
	- 两种不同的方法估计段落的真实性：
		- 与wikipedia比较
		- 使用校准模型不确定性
