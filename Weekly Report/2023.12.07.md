- 本周主要了解RAG相关的知识
	- Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection
		- 通过检索和自我反思来提高LM的质量和真实性
		- 实验 https://selfrag.github.io/ 没跑起来，vllm和fast-attn对torch版本冲突
			- vllm：torch >= 2.0.0
			- fast-attn：torch >= 1.13.0 && torch < 2.0.0
		- 熟悉 vllm 框架
	- REPLUG: Retrieval-Augmented Black-Box Language Models
		- 一个检索增强语言建模框架，将语言模型(LM)视为黑盒，并以可微调的检索模型对其进行增强
		- 不使用特殊的交叉注意力机制来训练语言模型以编码检索得文本，只将检索到文档预加到冻结的黑盒语言模型的输入中
	- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
		- 将预先训练好的参数内存和非参数内存结合起来用于语言生成
		- RAG-Sequence Model：使用相同的检索文档来生成完整的序列。
		- RAG-Token Model：每个要生成的token都对应一个检索出的文档，即同一个句子的不同token使用不同的检索文档。
	- IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions
		- 提出了一个归纳增强生成（IAG）框架，该框架利用归纳知识以及检索到的文档进行隐式推理
		- IAG-GPT直接利用GPT-3生成的知识进行答案预测
		- IAG-Student通过结合学生**感应器**模型，在推理时摆脱了对GPT服务的依赖。
		- ![image.png|500](https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository/picgo/20231207161742.png)
- 下周安排
	- 想看看FID
	- 了解vllm
	- 目前LLM对长尾知识捕捉能力较弱
		- 扩大数据规模
		- 扩大模型规模
		- **添加辅助检索模块**
	- 
