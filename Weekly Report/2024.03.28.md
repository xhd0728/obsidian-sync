上周工作：
- 复现Gemini-pro在LitQA数据集下的Baseline

本周工作：
- 将知识库拆分成长度近似的text chunk，对每个chunk评估其对指定问题的相关程度
- 将相关程度较高的top1，top3，top5，top10取代full text，测试准确度
目前发现的问题：
- 将全文输入LLM，使用`langchain`的`RetrievalQA`能得到和论文中接近的结果
- 将全文按2000字符拆分为`chunk`，使用`PaperQA`中设计的tool，对每个chunk与question的**关联程度**进行打分，并返回分数最高的`top_k`个chunk，再使用这些精简的chunk检索
- example: ![](https://cdn.jsdelivr.net/gh/xhd0728/oss-github-picgo-repository@main/picgo/202403271706922.png)
- 按上述方法尝试使用`langchain`、`vllm`、`直接拼接prompt`三种方式，LLM的响应均为chunk的内容与问题关系不大，效果不好
- 由于LLM的输入限制，在输入论文文本时分别尝试将`chunk_size`设置为500，1000，2000
- 测试的LLM为`gpt-3.5-turbo`和`gemini-pro`
- embedding模型分别使用`openai/text-embedding-ada-002`，`BAAI/bge-large-zh-v1.5`，`gemini/embedding-001`
问题思考：
- LitQA中的问题是经过设计的，有时需要对全文内容进行多次`summary`
- 一个问题的答案可能在文中是跳跃式出现的