上周工作：
- 复现Gemini-pro在LitQA数据集下的Baseline

本周工作：
- 将知识库拆分成长度近似的text chunk，对每个chunk评估其对指定问题的相关程度
- 将相关程度较高的top1，top3，top5，top10取代full text，测试准确度
目前发现的问题：
- 将全文输入LLM，使用langchain的RetrievalQA能得到和论文中接近的结果
- 将全文按2000字符拆分为chunk，使用PaperQA中设计的tool，对每个chunk与question的关联程度进行打分，并返回分数最高的top_k个chunk，再使用这些精简的chunk检索
- ![image.png](https://proxy.kokomi0728.eu.org/proxy/https://github.com/xhd0728/oss-github-picgo-repository/picgo/202403271703594.png)

- 按上述方法尝试使用langchain、vllm、api三种方式，LLM的响应均为chunk的内容与问题关系不大，效果不好