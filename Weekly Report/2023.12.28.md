- 本周主要工作
	- Large Language Models Understand and Can be Enhanced by **Emotional Stimuli**
	- Take a **Step Back**: Evoking Reasoning via Abstraction in Large Language Models
- 一点想法：
	- 论文中探究【增加】不同类型的情绪化的提示，增加了模型解决问题的能力
	- 假设LLM不会推理，而是根据某些词激发出记忆过的知识
		- 1. 情绪化的句子起作用？ OR  某些情感较强的词起作用？
		- 2. 某些更加准确，更加有约束力的词起作用？
	- 实验：
		- 使用MMLU的college_physics和sociology数据集，Agent不保留history
			- 输入原问题，获取直接响应答案$a_1$
			- 随机替换原问题中的3个词语为其同义词，获取替换后的响应答案$a_2$
			- 随机替换原问题中的5个词语为其同义词，获取替换后的响应答案$a_3$
			- 将问题重复两次，获取响应答案$a_4$
			- 将问题重复两次，获取响应答案$a_5$
		- 分别计算ACC
		- [google doc](https://docs.google.com/spreadsheets/d/1UdHcoh4dU7Oa7syR5WN13vz4IhYmerFhZdTEZ-kAEvY/edit?usp=sharing)
	- 结果：
		- 将原问题随机替换token->正确率降低
		- 将原问题重复->正确率有增加
	- 