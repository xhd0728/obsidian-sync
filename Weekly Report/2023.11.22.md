
- 读论文 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- 读论文 RoBERTa: A Robustly Optimized BERT Pretraining Approach
- 读论文 BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension
- 读论文 Improving Language Understanding by Generative Pre-Training